{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "from workalendar.europe import France\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "\n",
    "\n",
    "from statsmodels.tsa.stattools import acf, pacf\n",
    "from copy import deepcopy\n",
    "\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "import plotly.graph_objs as go\n",
    "init_notebook_mode(connected=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions :\n",
    "\n",
    "\"\"\"Plots a simple serie in PLOTLY.\"\"\"\n",
    "def jsplot(dates , values , mode = 'lines+markers'):\n",
    "\n",
    "    data = [go.Scatter(\n",
    "              x=dates,\n",
    "              y=values,\n",
    "              mode = mode)]\n",
    "\n",
    "    iplot(data)\n",
    "    \n",
    "    \n",
    "\"\"\"Plot multiple series in PLOTLY:\"\"\"\n",
    "def jsplot_multiple(dates , values , mode = 'lines+markers'):\n",
    "\n",
    "    data = []\n",
    "    for col in values.columns:\n",
    "        splot = go.Scatter(\n",
    "                        x=dates,\n",
    "                        y=values[col],\n",
    "                        mode = mode,\n",
    "                        name = str(col) )\n",
    "        data.append(splot)\n",
    "\n",
    "    iplot(data)\n",
    "    \n",
    "    \n",
    "\"\"\"Function that test the stationarity of a Time series by\n",
    "computing and plotting rolling statistics, and then by performing\n",
    "An augmented Dickey Fuller test.\"\"\" \n",
    "\n",
    "def test_stationarity(timeseries , window = 50):\n",
    "    #Determing rolling statistics\n",
    "    #ce ci est l'ancienne version de pandas rolmean = pd.rolling_mean(timeseries, window=window)\n",
    "    rolmean = pd.Series(timeseries).rolling(window=window).mean()\n",
    "    #rolstd = pd.rolling_std(timeseries, window=window)\n",
    "    rolstd = pd.Series(timeseries).rolling(window=window).std()\n",
    "\n",
    "    #Plot rolling statistics:\n",
    "    fig = plt.figure(figsize=(12, 8))\n",
    "    orig = plt.plot(timeseries,label='Original')\n",
    "    mean = plt.plot(rolmean, color='red' , label='Rolling Mean')\n",
    "    std = plt.plot(rolstd, label = 'Rolling Std')\n",
    "    plt.legend(loc='best')\n",
    "    plt.title('Rolling Mean & Standard Deviation')\n",
    "    plt.show()\n",
    "    \n",
    "    #Perform Dickey-Fuller test:\n",
    "    print('Results of Dickey-Fuller Test:')\n",
    "    try:\n",
    "        dftest = adfuller(timeseries.dropna(), autolag='AIC')\n",
    "        dfoutput = pd.Series(dftest[0:4], index=['Test Statistic','p-value','#Lags Used','Number of Observations Used'])\n",
    "        for key,value in dftest[4].items():\n",
    "            dfoutput['Critical Value (%s)'%key] = value\n",
    "        print(dfoutput) \n",
    "    except:\n",
    "        print('test failed')\n",
    "        \n",
    "\n",
    "        \n",
    "\"\"\"Performs Acp - Pacp Analysis on a time serie.\"\"\"\n",
    "def acp_pacp(timeseries , nlags = 50):\n",
    "    lag_acf = acf(timeseries, nlags=nlags)\n",
    "    lag_pacf = pacf(timeseries, nlags=nlags, method='ols')\n",
    "    \n",
    "    print('lag_acf')\n",
    "    fig = plt.figure(figsize=(7 , 6))\n",
    "\n",
    "    sns.barplot( np.arange(len(lag_acf)) , lag_acf , palette = 'GnBu_d')\n",
    "    \n",
    "    \n",
    "    plt.axhline(y=0,linestyle='--',color='gray')\n",
    "    plt.axhline(y=-1.96/np.sqrt(len(timeseries)),linestyle='--',color='gray')\n",
    "    plt.axhline(y=1.96/np.sqrt(len(timeseries)),linestyle='--',color='gray')\n",
    "\n",
    "    plt.show()\n",
    "    print('lag_pacf')\n",
    "    fig = plt.figure(figsize=(7, 6))\n",
    "\n",
    "    sns.barplot( np.arange(len(lag_pacf)) , lag_pacf , palette = 'GnBu_d')\n",
    "\n",
    "    plt.axhline(y=0,linestyle='--',color='gray')\n",
    "    plt.axhline(y=-1.96/np.sqrt(len(timeseries)),linestyle='--',color='gray')\n",
    "    plt.axhline(y=1.96/np.sqrt(len(timeseries)),linestyle='--',color='gray')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Importing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "File b'/home/fitec/Bureau/Project/accidents-in-france-from-2005-to-2016/caracteristics.csv' does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-614e9588beee>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mcaracs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'/home/fitec/Bureau/Project/accidents-in-france-from-2005-to-2016/caracteristics.csv'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlow_memory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'latin-1'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# Caracteristics of the accidents.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mplaces\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'/home/fitec/Bureau/Project/accidents-in-france-from-2005-to-2016/places.csv'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlow_memory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m \u001b[1;33m)\u001b[0m \u001b[1;31m# Places features.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0musers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'/home/fitec/Bureau/Project/accidents-in-france-from-2005-to-2016/users.csv'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlow_memory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m \u001b[1;33m)\u001b[0m \u001b[1;31m# Users involved in the accdient features.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mvehicles\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'/home/fitec/Bureau/Project/accidents-in-france-from-2005-to-2016/vehicles.csv'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlow_memory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# Vehicles features.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, doublequote, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    676\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[0;32m    677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 678\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    679\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    680\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    438\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    439\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 440\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    441\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    442\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    785\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    786\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 787\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    788\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    789\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1012\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'c'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1013\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'c'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1014\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1015\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1016\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'python'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1706\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'usecols'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1707\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1708\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1709\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1710\u001b[0m         \u001b[0mpassed_names\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnames\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: File b'/home/fitec/Bureau/Project/accidents-in-france-from-2005-to-2016/caracteristics.csv' does not exist"
     ]
    }
   ],
   "source": [
    "caracs = pd.read_csv('/home/fitec/Bureau/Project/accidents-in-france-from-2005-to-2016/caracteristics.csv', low_memory = False, encoding = 'latin-1') # Caracteristics of the accidents.\n",
    "places = pd.read_csv('/home/fitec/Bureau/Project/accidents-in-france-from-2005-to-2016/places.csv', low_memory = False ) # Places features.\n",
    "users = pd.read_csv('/home/fitec/Bureau/Project/accidents-in-france-from-2005-to-2016/users.csv', low_memory = False ) # Users involved in the accdient features.\n",
    "vehicles = pd.read_csv('/home/fitec/Bureau/Project/accidents-in-france-from-2005-to-2016/vehicles.csv', low_memory = False) # Vehicles features.\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Importing holidays in France"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Holidays in France :\n",
    "cal = France()\n",
    "holidays = []\n",
    "for year in range(2005,2017):\n",
    "    holidays.extend(cal.holidays(year))\n",
    "    \n",
    "holidays = pd.DataFrame(holidays , columns = ['ds' , 'holiday'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtsers = caracs.loc[(caracs.dep.isin([750])) , ['Num_Acc' , 'jour' , 'mois' , 'an']]\n",
    "\n",
    "dtsers['day'] = pd.to_datetime((2000+dtsers.an)*10000+dtsers.mois*100+dtsers.jour,format='%Y%m%d')\n",
    "dtsers.drop(['jour' , 'mois' , 'an'] , axis = 1 ,inplace = True)\n",
    "\n",
    "\n",
    "dtsers = dtsers.groupby('day' , as_index = False).count()\n",
    "\n",
    "# Dummy Variable Holiday\n",
    "dtsers['isholiday'] = 0\n",
    "dtsers.loc[dtsers.day.isin(holidays.ds) , 'isholiday'] = 1\n",
    "\n",
    "# Week day and month\n",
    "dtsers['weekday'] = dtsers.day.dt.weekday\n",
    "dtsers['month'] = dtsers.day.dt.month\n",
    "# Dummification\n",
    "dtsers = pd.get_dummies(dtsers , columns = ['weekday' , 'month'])\n",
    "\n",
    "print(' the 3 last years of the time series:')\n",
    "jsplot(dtsers.day[3500:] , dtsers.Num_Acc[3500:] )"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "There doesn’t seem to be any particular trend in the time series. But we can notice seasonal patterns that will be of great value to our model and a high number of spikes that we can try to explain to gain more insights into the problem. Let’s take a closer look at all that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time series Analysis :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quelques statistiques \n",
    "Commençons par faire quelques études statistiques: telles que le calcul de la moyenne et l’écart-type, puis utilisons le test statistique de (Dickey-Fuler) pour déterminer si la série est stationnaire.La stationnarité d'une série chronologique est importante pour la modélisation. Certains modèles ne peuvent fonctionner qu'avec des séries temporelles stationnaires.Ceci donne aussi une bonne comprehension de la structure de la série chronologique: cette analyse nous aide à détecter les tendances, les ruptures structurelles et tout type de tendance anormale qui peut nous aider à mieux comprendre et à adapter nos modèles et à mieux les généraliser. "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Dickey-fuller Test :This is one of the statistical tests for checking stationarity.\n",
    "First we consider the null hypothesis: the time series is non- stationary.\n",
    "The result from the rest will contain the test statistic and critical value for different confidence levels. \n",
    "The idea is to have Test statistics less than critical value, in this case we can reject the null hypothesis \n",
    "and say that this Time series is indeed stationary (the force is strong with this one !!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some statistics :\n",
    "test_stationarity(dtsers.Num_Acc , window = 28)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acp_pacp(dtsers.Num_Acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Running the example creates a 2D plot showing the lag value ((past period) ) along the x-axis and \n",
    "#the correlation on the y-axis between -1 and 1.\n",
    "from pandas import Series\n",
    "from matplotlib import pyplot\n",
    "from statsmodels.graphics.tsaplots import plot_acf\n",
    "plot_acf(dtsers.Num_Acc,lags=50)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Rq:On the graph, there is a vertical line (a “spike”) corresponding to each lag. \n",
    "The height of each spike shows the value of the autocorrelation function for the lag.\n",
    "\n",
    "The autocorrelation with lag zero always equals 1, because this represents the autocorrelation between each term \n",
    "and itself. Number of accidents and number of accidents with lag zero are the same variable.\n",
    "\n",
    "Each spike that rises above or falls below the dashed lines is considered to be statistically significant.\n",
    "If a spike is significantly different from zero, that is evidence of autocorrelation. \n",
    "A spike that’s close to zero is evidence against autocorrelation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dtsers' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-adf246af79b5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mstatsmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraphics\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtsaplots\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mplot_pacf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mplot_pacf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mNum_Acc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlags\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mpyplot\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'dtsers' is not defined"
     ]
    }
   ],
   "source": [
    "from statsmodels.graphics.tsaplots import plot_pacf\n",
    "plot_pacf(dtsers.Num_Acc, lags=50)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "These plots confirm that there is strong seasonality in our data. \n",
    "We can clearly see that we have here a weekly seasonality, thus a pattern repeating every week.\n",
    "This kind of pattern is valuable because it will allow us to improve our prediction based on what happened \n",
    "last week for example. We can also notice a strong correlation with what happened the day before.\n",
    "\n",
    "Let’s track more kinds of seasonality:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hourly Seasonality\n",
    "\n",
    "This kind of seasonality won’t be of any use to our model, but it’s good to know in case we want to build an hourly model instead of a daily model. We will simply sum the number of accidents per hour after distinguishing grave accidents (Death or grave injury) from the rest and see what we can find:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'caracs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-7a903aabd44b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtempas\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcaracs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcaracs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdep\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m750\u001b[0m \u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'Num_Acc'\u001b[0m \u001b[1;33m,\u001b[0m \u001b[1;34m'hrmn'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mtempas\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'hour'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtempas\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'hrmn'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzfill\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mgrave_accs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0musers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0musers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrav\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mNum_Acc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'caracs' is not defined"
     ]
    }
   ],
   "source": [
    "tempas = caracs.loc[caracs.dep == 750 , ['Num_Acc' , 'hrmn']]\n",
    "tempas['hour'] = tempas['hrmn'].apply(lambda x:str(x).zfill(4)[:2])\n",
    "\n",
    "\n",
    "grave_accs = users[users.grav.isin([2,3]) ].Num_Acc\n",
    "\n",
    "tempas['gravity'] = 0\n",
    "tempas.loc[tempas.Num_Acc.isin(grave_accs),'gravity'] = 1\n",
    "\n",
    "\n",
    "occs = tempas.drop('hrmn' , axis = 1).groupby('hour' , as_index = False).agg({'Num_Acc' : 'count' , 'gravity' : 'sum'})\n",
    "\n",
    "trace1 = go.Area(\n",
    "    r=list(occs.Num_Acc),\n",
    "    t=list(occs.hour),\n",
    "    name='Total Number of accidents',\n",
    "    marker=dict(\n",
    "        color='blue'\n",
    "    )\n",
    ")\n",
    "\n",
    "trace2 = go.Area(\n",
    "    r=list(occs.gravity),\n",
    "    t=list(occs.hour),\n",
    "    name='Grave accidents',\n",
    "    marker=dict(\n",
    "        color='red'\n",
    "    )\n",
    ")\n",
    "\n",
    "data = [trace1 , trace2]\n",
    "\n",
    "layout = go.Layout(\n",
    "    title='Repartition of accidents per Hour',\n",
    "    autosize = False,\n",
    "    width = 1000,\n",
    "    height = 500,\n",
    "    orientation=-90\n",
    ")\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "#iplot(fig)\n",
    "iplot(fig,filename=\"legend-names\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "We can clearly notice two spikes, one at 09h00, the time people go to work and another one between 17h00 and 18h00, time when people return home. The number of accidents decreases between these two spikes Nothing unusual but it proves there is a pattern here.\n",
    "\n",
    "We can also see that there are few grave accidents in Paris since there re only in-town roads."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Daily Seasonality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'caracs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-28a2c7192053>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtempas\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcaracs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcaracs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdep\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m750\u001b[0m \u001b[1;33m,\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Num_Acc'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mtempas\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'date'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_datetime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2000\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mcaracs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0man\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m10000\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mcaracs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmois\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mcaracs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjour\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'%Y%m%d'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mtempas\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'weekday'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtempas\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'date'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweekday\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzfill\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mtempas\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'gravity'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'caracs' is not defined"
     ]
    }
   ],
   "source": [
    "tempas = caracs.loc[caracs.dep == 750 ,['Num_Acc']]\n",
    "tempas['date'] = pd.to_datetime((2000+caracs.an)*10000+caracs.mois*100+caracs.jour,format='%Y%m%d')\n",
    "tempas['weekday'] = tempas['date'].dt.weekday.apply(lambda x:str(x).zfill(2))\n",
    "\n",
    "tempas['gravity'] = 0\n",
    "tempas.loc[tempas.Num_Acc.isin(grave_accs),'gravity'] = 1\n",
    "\n",
    "\n",
    "occs = tempas.drop('date' , axis = 1).groupby('weekday' , as_index = False).agg({'Num_Acc' : 'count' , 'gravity' : 'sum'})\n",
    "\n",
    "\n",
    "\n",
    "trace1 = go.Area(\n",
    "    r=list(occs.Num_Acc),\n",
    "    t=list(occs.weekday),\n",
    "    name='Total Number of accidents',\n",
    "    marker=dict(\n",
    "        color=\"blue\"\n",
    "    )\n",
    ")\n",
    "\n",
    "trace2 = go.Area(\n",
    "    r=list(occs.gravity),\n",
    "    t=list(occs.weekday),\n",
    "    name='Grave accidents',\n",
    "    marker=dict(\n",
    "        color=\"red\"\n",
    "    )\n",
    ")\n",
    "\n",
    "data = [trace1 , trace2]\n",
    "\n",
    "layout = go.Layout(\n",
    "    title='Repartition of accidents per weekday',\n",
    "    autosize = False,\n",
    "    width = 1000,\n",
    "    height = 500,\n",
    "    orientation=-90\n",
    ")\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "iplot(fig)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "We can clearly notice a pattern, where the number of accidents decrease in Fridays and Sundays and reach its minimum in Saturdays. The other days are roughly the same (working days)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monthly Seasonality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'caracs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-548a33d5b51b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtempas\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcaracs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcaracs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdep\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m750\u001b[0m \u001b[1;33m,\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Num_Acc'\u001b[0m \u001b[1;33m,\u001b[0m \u001b[1;34m'mois'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mtempas\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'mois'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtempas\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'mois'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzfill\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mtempas\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'gravity'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mtempas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtempas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mNum_Acc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrave_accs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'gravity'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'caracs' is not defined"
     ]
    }
   ],
   "source": [
    "tempas = caracs.loc[caracs.dep == 750 ,['Num_Acc' , 'mois']]\n",
    "tempas['mois'] = tempas['mois'].apply(lambda x:str(x).zfill(2))\n",
    "\n",
    "tempas['gravity'] = 0\n",
    "tempas.loc[tempas.Num_Acc.isin(grave_accs),'gravity'] = 1\n",
    "\n",
    "\n",
    "occs = tempas.groupby('mois' , as_index = False).agg({'Num_Acc' : 'count' , 'gravity' : 'sum'})\n",
    "\n",
    "\n",
    "\n",
    "trace1 = go.Area(\n",
    "    r=list(occs.Num_Acc),\n",
    "    t=list(occs.mois),\n",
    "    name='Total Number of accidents',\n",
    "    marker=dict(\n",
    "        color=\"blue\"\n",
    "    )\n",
    ")\n",
    "\n",
    "trace2 = go.Area(\n",
    "    r=list(occs.gravity),\n",
    "    t=list(occs.mois),\n",
    "    name='Grave accidents',\n",
    "    marker=dict(\n",
    "        color=\"red\"\n",
    "    )\n",
    ")\n",
    "\n",
    "data = [trace1 , trace2]\n",
    "\n",
    "layout = go.Layout(\n",
    "    title='Repartition of accidents per Hour',\n",
    "    autosize = False,\n",
    "    width = 1000,\n",
    "    height = 500,\n",
    "    orientation=-90\n",
    ")\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "iplot(fig)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Again, we can see an anual pattern in the time series, strongly related to the time go on vacation (August, December, …).\n",
    "\n",
    "By incorporating these patterns in our models, we will be able to make better predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling:\n",
    "To score our models, we will use this function that computes a set of meaningful statistics for the predictions we make. We will backtest our model for the last year and see how it performs.\n",
    "\n",
    "The function below will compute the Mean Error, Mean absolute error (MAE), the Root mean squared error (RMSE), the Mean Percentage error (MPE), the correlation between the true value and the error to see if there is any information left. It will also plot the distribution of the error and the prediction versus the true value according to time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(y_true , y_pred , dates):\n",
    "    \n",
    "    try:\n",
    "        true_value , prediction = y_true.sum(axis = 1), y_pred.sum(axis=1).round()\n",
    "    except:\n",
    "        true_value , prediction = y_true, y_pred.round()\n",
    "    \n",
    "    print('Mean Absolute Error   :' , round(abs(true_value - prediction).mean() , 2)) \n",
    "    print('Root Mean Square Error:' , round(sqrt(((true_value - prediction)**2).mean()) , 2) )\n",
    "    print('Mean Percentage Error :' , round((abs(true_value - prediction)/true_value).mean() , 2)  )\n",
    "    \n",
    "    error = pd.Series(true_value - prediction)\n",
    "    \n",
    "    #density plot :\n",
    "    print('Error Density :')\n",
    "    error.plot.density()\n",
    "    plt.show()\n",
    "    \n",
    "    # mean of error and correlation :\n",
    "    print('Mean Error                       :' , round(mean(error) , 2 ))\n",
    "    print('True Value And error Correlation :' , round(np.corrcoef(error , true_value)[0 , 1] , 2))\n",
    "    \n",
    "    # plot :\n",
    "    \n",
    "    to_plot = pd.DataFrame({'target' : y_true , 'prediction' : y_pred})\n",
    "    \n",
    "    jsplot_multiple(dates , to_plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naïve Model:\n",
    "Before going further, we need to build a ‘Naïve’ model that will represent the minimum acceptable performance in order to benchmark the models we are going to build. This model must be the simplest and more intuitive possible.\n",
    "\n",
    "Since we have strong seasonality patterns and no trends, we will take the number of accidents last year in the same day the same week as the prediction for this year, and see what performances we get:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dtsers' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-ced61ebf9695>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Naive Model :\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m new , old = (dtsers.loc[dtsers.day.dt.year == 2016 , ['day' , 'Num_Acc']].reset_index(drop = True) ,\n\u001b[0m\u001b[0;32m      4\u001b[0m              dtsers.loc[dtsers.day.dt.year == 2015 , ['day' , 'Num_Acc']].reset_index(drop = True)[:365])\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'dtsers' is not defined"
     ]
    }
   ],
   "source": [
    "# Naive Model :\n",
    "\n",
    "new , old = (dtsers.loc[dtsers.day.dt.year == 2016 , ['day' , 'Num_Acc']].reset_index(drop = True) ,\n",
    "             dtsers.loc[dtsers.day.dt.year == 2015 , ['day' , 'Num_Acc']].reset_index(drop = True)[:365])\n",
    "\n",
    "old.columns = ['day' , 'old']\n",
    "\n",
    "new['weekofyear'] , new['dayofweek'] = new.day.dt.weekofyear , new.day.dt.dayofweek\n",
    "old['weekofyear'] , old['dayofweek'] = old.day.dt.weekofyear , old.day.dt.dayofweek\n",
    "\n",
    "merged = new.merge(old , on = ['weekofyear' , 'dayofweek'])\n",
    "\n",
    "\n",
    "evaluate(merged.Num_Acc , merged.old , dtsers.day[-365:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prophet:\n",
    "We will first try the Prophet library, made by Facebook. Prophet is a forecasting tool developed by Facebook for its capacity planning. It is Highly efficient when it comes to human-scale problems. It is based on Generalized additive models.\n",
    "\n",
    "The Library itself is easily usable since the model will parameter itself for the most part. It also takes directly raw dates and Holidays without the need of preprocessing those variables.\n",
    "\n",
    "Let’s try this model on our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'fbprophet'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-f503e9c6cf11>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mfbprophet\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mProphet\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'fbprophet'"
     ]
    }
   ],
   "source": [
    "from fbprophet import Prophet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Prophet' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-f7bcfce2f2d6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#Initialisation of the model.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mProphet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mholidays\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mholidays\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0myearly_seasonality\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mweekly_seasonality\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdaily_seasonality\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m#train & test set.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m histo , new = dtsers[dtsers.day.dt.year < 2016].reset_index(drop = True) , dtsers[dtsers.day.dt.year \n",
      "\u001b[1;31mNameError\u001b[0m: name 'Prophet' is not defined"
     ]
    }
   ],
   "source": [
    "#Initialisation of the model.\n",
    "model = Prophet(holidays = holidays , yearly_seasonality=True , weekly_seasonality=True, daily_seasonality=False)\n",
    "\n",
    "#train & test set.\n",
    "histo , new = dtsers[dtsers.day.dt.year < 2016].reset_index(drop = True) , dtsers[dtsers.day.dt.year \n",
    "                                                                                  == 2016].reset_index(drop = True)\n",
    "\n",
    "# We rename the columns before fitting the model to Prophet.\n",
    "ncols = histo.columns.values\n",
    "ncols[0] , ncols[1] = 'ds' , 'y'\n",
    "\n",
    "histo.columns , new.columns = ncols , ncols\n",
    "\n",
    "# We fit the model.\n",
    "model.fit(histo)\n",
    "\n",
    "\n",
    "# Prediction\n",
    "ypred = model.predict(new)['yhat'].round()\n",
    "print(\"yest\")\n",
    "print(\"type\", type(ypred))\n",
    "print(ypred)\n",
    "\n",
    "# Evaluation\n",
    "evaluate(new.y , ypred , dtsers.day[-365:])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "We can see a clear improvement compared to the Naïve model. we were able to reduce the Mean Percentage error by 7% and we also improved the other metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTMs:\n",
    "Let's now try LSTMs (Long short-term memory networks). this architecture is adapted to sequence models and is usually used for NLP tasks, but we can also use it for time series modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'keras'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-ee562515db51>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSequential\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mload_model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDense\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mLSTM\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDropout\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mConv1D\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mMaxPooling1D\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mReshape\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mActivation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mMasking\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mTimeDistributed\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mBidirectional\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msequence\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mTimeseriesGenerator\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'keras'"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Sequential , load_model\n",
    "from keras.layers import Dense , LSTM, Dropout , Conv1D , MaxPooling1D , Reshape , Activation\n",
    "from keras.layers import Masking , TimeDistributed, Bidirectional\n",
    "from keras.preprocessing.sequence import TimeseriesGenerator\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from keras.callbacks import History , ModelCheckpoint"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "First, We need to reshape our data we have from a 2D input to a 3D input.\n",
    "\n",
    "To model our problem. We will take small windows from our time series and predict the last values with the previous values (lags). This way we will have multiple small sequences rather than a single big sequence.\n",
    "\n",
    "The input the LSTM needs is as the following: (batch_size, timesteps, features). Timesteps is the number of steps in our sequence (the window size in our case) and features is the number of features per step (lag + other features).\n",
    "\n",
    "We will suppose that we have the weather features for the day we want to predict, so we will add these data to the sequence with a lag of -2 to keep the same shape.\n",
    "\n",
    "The function below applies all these transformations to a time series and scales the output befoe fitting it to our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape_timeseries(series , target_ids, window_size , take_curr = True , scale = True):\n",
    "    \n",
    "    \n",
    "    # Converting the dataset to a suitable format :\n",
    "    X = series.values\n",
    "    Y = series.iloc[ : , target_ids].values\n",
    "    \n",
    "    # Scaling the data  \n",
    "    if scale:\n",
    "        maxes = Y.max(axis = 0)\n",
    "        Y = np.divide( Y , maxes)\n",
    "        X = MinMaxScaler().fit_transform(X)\n",
    "    \n",
    "    # Conversion to time series with keras object\n",
    "    ts = TimeseriesGenerator(X , Y , length = window_size , batch_size = X.shape[0])\n",
    "    X , Y = ts[0]\n",
    "    \n",
    "    # Masking\n",
    "    if take_curr:\n",
    "        for timestep in X[: , window_size - 1]:\n",
    "            timestep[target_ids] = [-2 for i in target_ids]\n",
    "    else:\n",
    "        X = X[: , :-1]\n",
    "        \n",
    "    if scale:\n",
    "        return X , Y , maxes\n",
    "\n",
    "    return X,Y"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Now let's build our model. The function below constructs a Keras sequential model by stacking a Masking layer, multiple BI-LSTM layers and multiple Dense Layers (number of layers and number of neurons per layer, as well as Dropout per layer are set by the user)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X , Y , lr = 0.001,\n",
    "          lstm_layers = [] , lstm_dropout = [],\n",
    "          dense_layers = [] , dense_dropout = [] ,\n",
    "          ntest_day = 365 , epochs = 10 , batch_size = 32):\n",
    "        \n",
    "        \n",
    "    # training and testing set :\n",
    "    length , timesteps , features = X.shape[0] , X.shape[1] , X.shape[2]\n",
    "    target_shape = Y.shape[1]\n",
    "    \n",
    "    # Validation rate to pass to the Sequential Model :\n",
    "    val_rate = ntest_day/length\n",
    "    \n",
    "    \n",
    "    ############################################ Model :\n",
    "    \n",
    "    checkpoint = ModelCheckpoint('model' , save_best_only=True)\n",
    "    \n",
    "    model = Sequential()\n",
    "    \n",
    "    # Masking Layer.\n",
    "    model.add(Masking(mask_value = -2 , input_shape=(X.shape[1],  X.shape[2])    ))\n",
    "    \n",
    "    \n",
    "    # BI-LSTM Layers.\n",
    "    for i in range(len(lstm_layers)):\n",
    "        rsequs  = not (i == (len(lstm_layers) - 1))\n",
    "        model.add(Bidirectional( LSTM(lstm_layers[i] , return_sequences = rsequs) ,input_shape=(X.shape[1], X.shape[2]) ) )\n",
    "        model.add(Dropout(lstm_dropout[i]))\n",
    "\n",
    "\n",
    "    # Dense Layers.\n",
    "    for i in range(len(dense_layers)): \n",
    "        model.add(Dense(dense_layers[i]) )\n",
    "        model.add(Dropout(dense_dropout[i]))\n",
    "        model.add(Activation('relu'))\n",
    "        \n",
    "    \n",
    "    model.add(Dense(target_shape))\n",
    "    Nadam = keras.optimizers.Nadam(lr = lr , beta_1=0.9, beta_2=0.999, epsilon=1e-08)#, schedule_decay=0.0004)\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    \n",
    "    print('Model Summary:')\n",
    "    print(model.summary())\n",
    "    \n",
    "    # fitting the data\n",
    "    print('\\n\\n Training :')\n",
    "    model.fit(X, Y, epochs= epochs, batch_size=batch_size, validation_split = val_rate, callbacks = [checkpoint])\n",
    "    \n",
    "    \n",
    "    # loading best_model\n",
    "    model = load_model('model')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dtsers' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-ccda02a10210>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mX\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mmaxes\u001b[0m  \u001b[1;33m=\u001b[0m \u001b[0mreshape_timeseries\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwindow_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m28\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mtake_curr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mscale\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mntest_day\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m365\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m nmodel = model(X , Y , lr = 0.002, lstm_layers = [20 ] , lstm_dropout = [.3 ] ,\n\u001b[0;32m      5\u001b[0m                dense_layers = [500] , dense_dropout = [.5] , batch_size = 64 , epochs = 20)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'dtsers' is not defined"
     ]
    }
   ],
   "source": [
    "X , Y , maxes  = reshape_timeseries(dtsers.iloc[:, 1:] , [0], window_size = 28 , take_curr = True , scale = True)\n",
    "ntest_day = 365\n",
    "\n",
    "nmodel = model(X , Y , lr = 0.002, lstm_layers = [20 ] , lstm_dropout = [.3 ] ,\n",
    "               dense_layers = [500] , dense_dropout = [.5] , batch_size = 64 , epochs = 20)\n",
    "\n",
    "# Computing Validation scores : MAE - RMSE - MPE\n",
    "y_predict = nmodel.predict(X[- ntest_day:]) * maxes\n",
    "y_true = Y[- ntest_day:] * maxes\n",
    "#\n",
    "#print(\"type:\", type(y_true))\n",
    "y_true = pd.Series(y_true.reshape(-1))\n",
    "y_predict = pd.Series(y_predict.reshape(-1))\n",
    "#print(y_true)\n",
    "evaluate(y_true , y_predict , dtsers.day[-365:])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The performances are roughly the same than Prophet. But the prediction made here is a day+1 prediction rather than a prediction of the whole year like we made with Prophet. So let's right a function that will predict the next year step by step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def long_term_prediction(model , X , nb_target):\n",
    "#Function also adapted to multiple targets    \n",
    "    predictions = []\n",
    "    new_line = X[0].reshape(1 , *X.shape[1:])\n",
    "    pred = model.predict(new_line)\n",
    "    predictions.append(pred)\n",
    "    \n",
    "    for line in X[1:]:\n",
    "        old_line = deepcopy(line)\n",
    "        old_line[-2 , :nb_target] = pred\n",
    "        pred= model.predict(old_line.reshape(1 , *X.shape[1:]))\n",
    "        predictions.append(pred)\n",
    "        \n",
    "    return np.array(predictions).reshape(-1 , nb_target )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nmodel' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-15f9fd9f7c21>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Computing Validation scores : MAE - RMSE - MPE\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0my_predict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlong_term_prediction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnmodel\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m \u001b[0mntest_day\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m \u001b[0mmaxes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0my_true\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m \u001b[0mntest_day\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mmaxes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0my_true\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSeries\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0my_predict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSeries\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_predict\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'nmodel' is not defined"
     ]
    }
   ],
   "source": [
    "# Computing Validation scores : MAE - RMSE - MPE\n",
    "y_predict = long_term_prediction(nmodel , X[- ntest_day:] , 1)* maxes\n",
    "y_true = Y[- ntest_day:] * maxes\n",
    "y_true = pd.Series(y_true.reshape(-1))\n",
    "y_predict = pd.Series(y_predict.reshape(-1))\n",
    "    \n",
    "evaluate(y_true , y_predict , dtsers.day[-365:])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "We can see that the results are stable and almost the same as Prophet. Let's try another way to model our problem with LSTMs:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multivariate Time series with LSTMs:\n",
    "What if we want to get an approximate location of future accidents ? This would be a highly valuable information if we want to take preventive measures. Paris is made up of 20 separate districts. Instead of predicting the sum of the total number of accidents, we are going to predict the number of accidents in each district in a multivariate way. This means that instead of outputting a single neuron, our model will output 20 neurons, each one representing a district. To score the model, we are going to sum the predictions and see if we get good results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'caracs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-9bc7917bc8f2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mcdtsers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcaracs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcaracs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdep\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m750\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'Num_Acc'\u001b[0m \u001b[1;33m,\u001b[0m \u001b[1;34m'dep'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'com'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'jour'\u001b[0m \u001b[1;33m,\u001b[0m \u001b[1;34m'mois'\u001b[0m \u001b[1;33m,\u001b[0m \u001b[1;34m'an'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mcdtsers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'day'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_datetime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2000\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mcdtsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0man\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m10000\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mcdtsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmois\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mcdtsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjour\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'%Y%m%d'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mcdtsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'jour'\u001b[0m \u001b[1;33m,\u001b[0m \u001b[1;34m'mois'\u001b[0m \u001b[1;33m,\u001b[0m \u001b[1;34m'an'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;33m,\u001b[0m\u001b[0minplace\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'caracs' is not defined"
     ]
    }
   ],
   "source": [
    "cdtsers = caracs.loc[(caracs.dep.isin([750])) , ['Num_Acc' , 'dep', 'com', 'jour' , 'mois' , 'an']]\n",
    "\n",
    "\n",
    "cdtsers['day'] = pd.to_datetime((2000+cdtsers.an)*10000+cdtsers.mois*100+cdtsers.jour,format='%Y%m%d')\n",
    "cdtsers.drop(['jour' , 'mois' , 'an'] , axis = 1 ,inplace = True)\n",
    "\n",
    "def correct(x):\n",
    "    if x>100:\n",
    "        return x - 100\n",
    "    return x\n",
    "\n",
    "cdtsers.com = cdtsers.com.apply( correct )\n",
    "\n",
    "cdtsers = cdtsers.groupby(['day' , 'dep' , 'com'] , as_index = False).count()\n",
    "\n",
    "cdtsers = cdtsers.pivot_table(index = ['day' , 'dep'] , columns = 'com' , values = 'Num_Acc').reset_index()\n",
    "\n",
    "cdtsers.fillna(0).head()\n",
    "\n",
    "cdtsers['isholiday'] = 0\n",
    "cdtsers.loc[cdtsers.day.isin(holidays.ds) , 'isholiday'] = 1\n",
    "\n",
    "\n",
    "\n",
    "cdtsers['weekday'] = cdtsers.day.dt.weekday\n",
    "cdtsers['month'] = cdtsers.day.dt.month\n",
    "cdtsers = pd.get_dummies(cdtsers , columns = ['weekday' , 'month'])\n",
    "\n",
    "\n",
    "cdtsers.drop([56 , 'dep'] , axis = 1 , inplace = True)\n",
    "cdtsers.fillna(0 , inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cdtsers' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-a1d6a7a9d5d0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mX\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mmaxes\u001b[0m  \u001b[1;33m=\u001b[0m \u001b[0mreshape_timeseries\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcdtsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m19\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwindow_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m28\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mtake_curr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mscale\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mntest_day\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m365\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m nmodel = model(X , Y , lr = 0.005, lstm_layers = [64 , 64] , lstm_dropout = [.2 , .2] ,\n",
      "\u001b[1;31mNameError\u001b[0m: name 'cdtsers' is not defined"
     ]
    }
   ],
   "source": [
    "X , Y , maxes  = reshape_timeseries(cdtsers.iloc[: , 1:] , list(range(19)), window_size = 28 , take_curr = True , scale = True)\n",
    "\n",
    "ntest_day = 365\n",
    "\n",
    "nmodel = model(X , Y , lr = 0.005, lstm_layers = [64 , 64] , lstm_dropout = [.2 , .2] ,\n",
    "               dense_layers = [64] , dense_dropout = [.2] , batch_size = 64 , epochs = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nmodel' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-bb119a852e85>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Evaluating:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0my_predict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mnmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m \u001b[0mntest_day\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mmaxes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0my_true\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mY\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m \u001b[0mntest_day\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mmaxes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0my_true\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSeries\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0my_predict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSeries\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_predict\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'nmodel' is not defined"
     ]
    }
   ],
   "source": [
    "# Evaluating:\n",
    "y_predict = (nmodel.predict(X[- ntest_day:]) * maxes).sum(axis = 1)\n",
    "y_true = (Y[- ntest_day:] * maxes).sum(axis = 1)\n",
    "y_true = pd.Series(y_true.reshape(-1))\n",
    "y_predict = pd.Series(y_predict.reshape(-1))\n",
    "\n",
    "evaluate(y_true , y_predict , cdtsers.day[-365:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nmodel' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-303e3d176598>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Evaluating on long term:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0my_predict\u001b[0m \u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0mlong_term_prediction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnmodel\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m \u001b[0mntest_day\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m,\u001b[0m \u001b[1;36m19\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m \u001b[0mmaxes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0my_true\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mY\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m \u001b[0mntest_day\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mmaxes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0my_true\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSeries\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0my_predict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSeries\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_predict\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'nmodel' is not defined"
     ]
    }
   ],
   "source": [
    "# Evaluating on long term:\n",
    "y_predict =( long_term_prediction(nmodel , X[- ntest_day:] , 19)* maxes).sum(axis = 1)\n",
    "y_true = (Y[- ntest_day:] * maxes).sum(axis = 1)\n",
    "y_true = pd.Series(y_true.reshape(-1))\n",
    "y_predict = pd.Series(y_predict.reshape(-1))\n",
    "    \n",
    "evaluate(y_true , y_predict , cdtsers.day[-365:])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "We get the same performance than the previous model, but we have an additional information which is the number of accidents per district.\n",
    "\n",
    "Conclusion & Next steps.¶\n",
    "Using the power of Prophet and LSTMs, we were already able to gain much performance compared to the Naïve model. But there is still room for improvement:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
